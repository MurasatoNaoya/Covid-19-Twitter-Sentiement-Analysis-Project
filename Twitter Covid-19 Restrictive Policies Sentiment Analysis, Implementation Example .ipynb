{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4fc0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The aim of this program is scrape data, or in this case, tweets from Twitter.\n",
    "# In the aim of getting a better understanding of the impression or 'sentiment' users have of the implementation of restrictive Covid-19 policies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea28c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the appropriate libraries - \n",
    "\n",
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import sys\n",
    "import re\n",
    "import matplotlib.pyplot as plt   # Here we used the 'as' keyword to create an alias for the module we want to import. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import nltk\n",
    "import pycountry \n",
    "import string \n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer  # [Talk about Vader, it's application(s) and how it can used alongside TextBlob.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f333cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, we need to earn access to the Twitter API through for this particular app(lication) -\n",
    "\n",
    "# Twitter API credentials - consumer API key, the consumer API secret, access token and access token secret. \n",
    "# The keys listed below are the keys for my particular Twitter Developer account, I probably shouldn't include them for security reasons. \n",
    "# But I can always take them out and if someone goes out of their way to use my application, then so be it!  \n",
    "Oauth1_consumer_key = \"vRo82DuZ4aHWEoMBMDgfYCyf4\"\n",
    "oauth1_consumer_secret = \"0beRltPHLJfiuJm9lAZHuIyozzDk8HFkrm6kJGCo7EmR10Ckl5\"\n",
    "oauth1_access_token = \"1454520951136194564-q20xpkUK7RFhqTivYw076vGzuqCxj1\"\n",
    "oauth1_access_token_secret = \"1PJGCmstBgPlLjgPTYBQMphhH60sQEfNvwDASlHgDrubR\"\n",
    "\n",
    "# Creating an instance of tweepy's .AuthHandler class - \n",
    "authentication = tweepy.OAuthHandler(Oauth1_consumer_key, oauth1_consumer_secret)\n",
    "\n",
    "# Setting the access token and access token secret - \n",
    "authentication.set_access_token(oauth1_access_token, oauth1_access_token_secret)\n",
    "\n",
    "# Creating the API object, that accounts for our authentication information - \n",
    "# The Twitter API has a rate limit of 900 requests per 15 minutes, it would return an error for anything above this amount. \n",
    "# The 'wait_on_rate_limit' parameter asks whether or not to automatically wait for rate limits to replenish, in this case we set it to 'True' to avoid any errors.  \n",
    "api = tweepy.API(authentication, wait_on_rate_limit= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd208371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter keyword or hashtag to focus your search: lockdown \n",
      "Please enter how many tweets you want to analyse: 100\n"
     ]
    }
   ],
   "source": [
    "# Now that we have authentication for our application, we can now use Tweepy, TextBlob and VADER to conduct sentiment analysis - \n",
    "\n",
    "# The below self-defined function will be used later to calculate the percentage of positive, negative and neutral Tweets we have in our sample.\n",
    "# The 'part' paramater representing whatever category of sentiment we want to look at and the 'whole' parameter representing our total number of Tweets - NoOfTweets. \n",
    "def percentage(part,whole):\n",
    "   return 100 * float(part)/float(whole)\n",
    "\n",
    "# Recording the keyword(s) and number of Tweets being considered\n",
    "keyword = input(\"Please enter keyword or hashtag to focus your search: \")\n",
    "NoOfTweets = int(input (\"Please enter how many tweets you want to analyse: \"))\n",
    "\n",
    "# Using tweepy to search and collect Tweets based on the predefined keyword(s) and number of Tweets we want to analyse - \n",
    "tweets = tweepy.Cursor(api.search_tweets, q = keyword).items(NoOfTweets) \n",
    "# Above we have defined what keyword(s) we want to analyse as well as the number of times this should be iterated. \n",
    "# The 'tweets' variable is just a list that contains all of the tweets from this iterative process of selecting related tweets. \n",
    "\n",
    "# Below are baseline counters that will be added to in order to keep track of the number of each tweet with different sentiment. \n",
    "# Where 'tweet_list' is all of the gathered text from tweets and all the others are a lists containing text from Tweets that have been sorted by sentiment -\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "tweet_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "\n",
    "# In the context of TextBlob, polarity indicates is a float in the range [-1-1], where 1 indicates a purely positive statement and -1 and purely negative statement.\n",
    "# This varibale will not be placeholder to count the number of Tweets of whatever specification. \n",
    "# It will be measured for each iteration to keep account for the extent to which a tweet is positive or negative. \n",
    "polarity = 0\n",
    "\n",
    "# A similar thing can be done for the subjectivity float score in the range of [0-1], \n",
    "# Where hypothetical a score of 0 represnts a purely objevtive statement and a score of 1 indicates a purely subjective statement. \n",
    "subjectivity = 0\n",
    "\n",
    "\n",
    "for tweet in tweets: # For every Tweet in the list of 'NoOfTweets' Tweets that our API has scraped, \n",
    "   tweet_list.append(tweet.text) # Add the Tweet text data to tweet_list \n",
    "\n",
    "   # We can now make our first 'TextBlob', which is necessary to able to apply our methods for sentiment analysis. \n",
    "   # This is in the for loop, so each individual tweet will be treated as a TextBlob to be processed. \n",
    "   analysis = TextBlob(tweet.text) # This analysis variable will be needed to calculate your polarity value for each individual tweet. \n",
    "   \n",
    "   # The polarity_scores method from the SentimentIntensityAnalyzer module, produces a dictionary of positive, negative, neutral and compound indexes.\n",
    "   # It is very important to note, that VADER and Textblob, although fulfilling similar purposes, do so in slightly different ways. \n",
    "   # VADER produces individual scores for each interpretation, as well as an aggreagate compound score. \n",
    "   # While Textblob provides only a singular polarity score (that can be compared to the compound score produced by VADER).\n",
    "   # As well as a subjectivity score, that aims to measure the degree of objectvity present in the statement can how opintionated it is. \n",
    "   # There are a few pros and cons for each library which are expanded on in the README file, but for the sake of whollsitic anaylsis, both will be used in this project. \n",
    "   \n",
    "   # That in this case, describe the sentiment of the scraped tweet. \n",
    "   score = SentimentIntensityAnalyzer().polarity_scores(tweet.text)\n",
    "   \n",
    "   # We can now assign the scores associated with the dictionary keys produced, to new variables. \n",
    "   neg = score['neg']\n",
    "   neu = score['neu']\n",
    "   pos = score['pos']\n",
    "   comp = score['compound']\n",
    "   \n",
    "   # +- is neccessary to add up all of the polarity score for whatever number of Tweets you analyse. \n",
    "   # This final score can later be divided by that same number of tweets and aggregated to produce an average polarity score for a specific topic. \n",
    "   polarity += analysis.sentiment.polarity \n",
    "   \n",
    "   # A similar thing can be done for our subjectivity score - \n",
    "   subjectivity += analysis.sentiment.subjectivity \n",
    "\n",
    "\n",
    "# Depending on the scores produced, we can now sort each Tweet into distinct categories and aggregate overall sentiment - \n",
    "   if neg > pos: \n",
    "      negative += 1 \n",
    "      negative_list.append(tweet)\n",
    "            \n",
    "   elif pos > neg: \n",
    "      positive += 1 \n",
    "      positive_list.append(tweet)\n",
    "      \n",
    "   elif neg == pos: \n",
    "      neutral += 1 \n",
    "      neutral_list.append(tweet)\n",
    "   \n",
    "   \n",
    "  \n",
    "positive_proportion = percentage(positive, NoOfTweets)\n",
    "negative_proportion = percentage(negative, NoOfTweets)\n",
    "neutral_proportion = percentage(neutral, NoOfTweets)\n",
    "aggregate_polarity = polarity / NoOfTweets \n",
    "aggregate_subjectivity = subjectivity / NoOfTweets\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b92d6742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.0\n",
      "35.0\n",
      "39.0\n",
      "-0.021557614838864838\n",
      "0.3869405964405964\n"
     ]
    }
   ],
   "source": [
    "print(positive_proportion)\n",
    "print(negative_proportion)\n",
    "print(neutral_proportion)\n",
    "print(aggregate_polarity)\n",
    "print(aggregate_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e931205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
