{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ebfac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The aim of this program is scrape data, or in this case, tweets from Twitter.\n",
    "# In the aim of getting a better understanding of the impression or 'sentiment' users have of the implementation of restrictive Covid-19 policies. \n",
    "\n",
    "\n",
    "# Import the appropriate libraries - \n",
    "\n",
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import sys\n",
    "import re\n",
    "import matplotlib.pyplot as plt   # Here we used the 'as' keyword to create an alias for the module we want to import. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer  # For this project, we are using both TextBlob and VADER for our sentiment analysis libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c35fb988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, we need to earn access to the Twitter API through for this particular app(lication) -\n",
    "\n",
    "\n",
    "# Twitter API credentials - consumer API key, the consumer API secret, access token and access token secret. \n",
    "# The below keys will likely be expried and will need to be regenerated via the Twitter Developer portal. \n",
    "Oauth1_consumer_key = \"OauYxLjFZVdjCYqXH6SQyFgz4\"\n",
    "oauth1_consumer_secret = \"Dwy9aPjQugtCEE1d066Q5ho12CQShhbArYSddpK7SHdnvRrNco\"\n",
    "oauth1_access_token = \"1454520951136194564-7wHZTZpELMeFz41TtOrrJJ0SBCpUjW\"\n",
    "oauth1_access_token_secret = \"q2hYeb0wYhCqIedXhrcPqN89atVdb8XUM8uen5hRwNRvn\"\n",
    "\n",
    "# Creating an instance of tweepy's .AuthHandler class - \n",
    "authentication = tweepy.OAuthHandler(Oauth1_consumer_key, oauth1_consumer_secret)\n",
    "\n",
    "# Setting the access token and access token secret - \n",
    "authentication.set_access_token(oauth1_access_token, oauth1_access_token_secret)\n",
    "\n",
    "# Creating the API object, that accounts for our authentication information - \n",
    "# The Twitter API has a rate limit of 900 requests per 15 minutes, it would return an error for anything above this amount. \n",
    "# The 'wait_on_rate_limit' parameter asks whether or not to automatically wait for rate limits to replenish, in this case we set it to 'True' to avoid any errors.  \n",
    "api = tweepy.API(authentication, wait_on_rate_limit= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12e879fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter keyword or hashtag to focus your search: lockdown \n",
      "Please enter how many tweets you want to analyse: 1000\n"
     ]
    }
   ],
   "source": [
    "# After authenticating our application, we can now use Tweepy, TextBlob and VADER to conduct sentiment analysis - \n",
    "\n",
    "# The below self-defined function will be used later to calculate the percentage of positive, negative and neutral Tweets we have in our sample.\n",
    "# Where the 'part' paramater represents whatever category of sentiment we are interested in and the 'whole' parameter represents our total number of Tweets - NoOfTweets. \n",
    "def percentage(part,whole):\n",
    "   return 100 * float(part)/float(whole)\n",
    "\n",
    "# Recording the keyword(s) and number of Tweets being considered\n",
    "keyword = input(\"Please enter keyword or hashtag to focus your search: \")\n",
    "NoOfTweets = int(input (\"Please enter how many tweets you want to analyse: \"))\n",
    "\n",
    "# Using tweepy to search and collect Tweets based on the predefined keyword(s) and number of Tweets we want to analyse - \n",
    "tweets = tweepy.Cursor(api.search_tweets, q = keyword).items(NoOfTweets) \n",
    "# Above we have defined what keyword(s) we want to analyse as well as the number of times this should be iterated. \n",
    "# The 'tweets' variable is just a list that contains all of the tweets from this iterative process of selecting related tweets. \n",
    "\n",
    "# Below are baseline counters that will be added to in order to keep track of the number of each tweet with different sentiment. \n",
    "# Where 'tweet_list' is all of the gathered text from tweets and all the others are a lists containing text from Tweets that have been sorted by sentiment -\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "tweet_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "\n",
    "# In the context of TextBlob, polarity indicates is a float in the range [-1-1], where 1 indicates a purely positive statement and -1 and purely negative statement.\n",
    "# This varibale will not be placeholder to count the number of Tweets of whatever specification. \n",
    "# It will be measured for each iteration to keep account for the extent to which a tweet is positive or negative. \n",
    "polarity = 0\n",
    "\n",
    "# A similar thing can be done for the subjectivity float score in the range of [0-1], \n",
    "# Where hypothetical a score of 0 represnts a purely objevtive statement and a score of 1 indicates a purely subjective statement. \n",
    "subjectivity = 0\n",
    "\n",
    "\n",
    "for tweet in tweets: # For every Tweet in the list of 'NoOfTweets' Tweets that our API has scraped, \n",
    "   tweet_list.append(tweet.text) # Add the Tweet text data to tweet_list \n",
    "\n",
    "   # We can now make our first 'TextBlob', which is necessary to able to apply our methods for sentiment analysis. \n",
    "   # This is in the for loop, so each individual tweet will be treated as a TextBlob to be processed. \n",
    "   analysis = TextBlob(tweet.text) # This analysis variable will be needed to calculate your polarity value for each individual tweet. \n",
    "   \n",
    "   # The polarity_scores method from the SentimentIntensityAnalyzer module, produces a dictionary of positive, negative, neutral and compound indexes.\n",
    "   # It is very important to note, that VADER and Textblob, although fulfilling similar purposes, do so in slightly different ways. \n",
    "   # VADER produces individual scores for each interpretation, as well as an aggreagate compound score. \n",
    "   # While Textblob provides only a singular polarity score (that can be compared to the compound score produced by VADER).\n",
    "   # As well as a subjectivity score, that aims to measure the degree of objectvity present in the statement can how opintionated it is. \n",
    "   # There are a few pros and cons for each library which are expanded on in the README file, but for the sake of whollsitic anaylsis, both will be used in this project. \n",
    "   \n",
    "   # That in this case, describe the sentiment of the scraped tweet. \n",
    "   score = SentimentIntensityAnalyzer().polarity_scores(tweet.text)\n",
    "   \n",
    "   # We can now assign the scores associated with the dictionary keys produced, to new variables. \n",
    "   neg = score['neg']\n",
    "   neu = score['neu']\n",
    "   pos = score['pos']\n",
    "   comp = score['compound']\n",
    "   \n",
    "   # +- is neccessary to add up all of the polarity score for whatever number of Tweets you analyse. \n",
    "   # This final score can later be divided by that same number of tweets and aggregated to produce an average polarity score for a specific topic. \n",
    "   polarity += analysis.sentiment.polarity \n",
    "   \n",
    "   # A similar thing can be done for our subjectivity score - \n",
    "   subjectivity += analysis.sentiment.subjectivity \n",
    "\n",
    "# Depending on the scores produced, we can now sort each Tweet into distinct categories and aggregate overall sentiment - \n",
    "   if neg > pos: \n",
    "      negative += 1 \n",
    "      negative_list.append(tweet)\n",
    "            \n",
    "   elif pos > neg: \n",
    "      positive += 1 \n",
    "      positive_list.append(tweet)\n",
    "      \n",
    "   elif neg == pos: \n",
    "      neutral += 1 \n",
    "      neutral_list.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a31ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_proportion = percentage(positive, NoOfTweets)\n",
    "negative_proportion = percentage(negative, NoOfTweets)\n",
    "neutral_proportion = percentage(neutral, NoOfTweets)\n",
    "aggregate_polarity = polarity / NoOfTweets \n",
    "aggregate_subjectivity = subjectivity / NoOfTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96afc461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.0\n",
      "28.8\n",
      "36.2\n",
      "0.08228390660034415\n",
      "0.3490587351676101\n"
     ]
    }
   ],
   "source": [
    "print(positive_proportion)\n",
    "print(negative_proportion)\n",
    "print(neutral_proportion)\n",
    "print(aggregate_polarity)\n",
    "print(aggregate_subjectivity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
